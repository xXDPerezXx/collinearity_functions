import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder

def calculate_mutual_info(X, y):
    """
    Calculate mutual information between features and with target variable.
    Handles both numerical and categorical features with proper encoding.
    """
    # Create a copy of the data to avoid modifying the original
    X_processed = X.copy()
    
    # Initialize label encoders for categorical columns
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
    numeric_cols = X.select_dtypes(exclude=['object']).columns.tolist()
    
    # Encode categorical variables
    encoders = {}
    for col in categorical_cols:
        encoders[col] = LabelEncoder()
        X_processed[col] = encoders[col].fit_transform(X_processed[col])
    
    # Initialize an empty matrix to store the mutual information scores
    n_features = X.shape[1]
    mi_matrix = np.zeros((n_features, n_features))
    feature_names = list(X.columns)
    
    # Calculate the mutual information scores between each pair of features
    for i, col1 in enumerate(feature_names):
        for j, col2 in enumerate(feature_names):
            if i == j:
                mi_matrix[i, j] = 1.0
            else:
                # Get the data for both columns
                x1 = X_processed[col1].values.reshape(-1, 1)
                x2 = X_processed[col2].values
                
                if col1 in categorical_cols and col2 in categorical_cols:
                    # Both categorical
                    mi_matrix[i, j] = mutual_info_classif(x1, x2, discrete_features=True)[0]
                elif col1 in numeric_cols and col2 in numeric_cols:
                    # Both numeric
                    mi_matrix[i, j] = mutual_info_regression(x1, x2)[0]
                elif col1 in categorical_cols and col2 in numeric_cols:
                    # First categorical, second numeric
                    est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
                    x2_discretized = est.fit_transform(x2.reshape(-1, 1)).ravel()
                    mi_matrix[i, j] = mutual_info_classif(x1, x2_discretized, discrete_features=True)[0]
                else:
                    # First numeric, second categorical
                    est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
                    x1_discretized = est.fit_transform(x1).ravel()
                    mi_matrix[i, j] = mutual_info_classif(x2.reshape(-1, 1), x1_discretized, discrete_features=True)[0]
    
    # Create a DataFrame with the mutual information scores
    mi_df = pd.DataFrame(mi_matrix, index=feature_names, columns=feature_names)
    
    # Create the visualization
    if len(feature_names) > 10:
        fig, ax = plt.subplots(figsize=(24, 22))    
    else:
        fig, ax = plt.subplots(figsize=(12, 10))
    
    # Create heatmap with a different colormap that's more distinguishable
    # im = ax.imshow(mi_df, cmap='viridis', aspect='auto')
    # im = ax.imshow(mi_df, cmap='coolwarm', aspect='auto')
    im = ax.imshow(mi_df, cmap='RdYlBu_r', aspect='auto')
    
    # Add colorbar
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel('Mutual Information Score', rotation=-90, va="bottom")
    
    # Show all ticks and label them
    ax.set_xticks(np.arange(len(feature_names)))
    ax.set_yticks(np.arange(len(feature_names)))
    ax.set_xticklabels(feature_names)
    ax.set_yticklabels(feature_names)
    
    # Rotate the tick labels and set their alignment
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # Loop over data dimensions and create text annotations
    for i in range(len(feature_names)):
        for j in range(len(feature_names)):
            text = ax.text(j, i, f'{mi_df.iloc[i, j]:.2f}',
                         ha="center", va="center", 
                         color="white" if mi_df.iloc[i, j] > 0.5 else "black")
    
    # Add title
    ax.set_title("Mutual Information Matrix")
    
    # Tight layout to prevent label cutoff
    fig.tight_layout()
    
    plt.show()
    
    # Calculate feature importances with target
    feature_importances = []
    for col in feature_names:
        x = X_processed[col].values.reshape(-1, 1)
        if col in categorical_cols:
            mi_score = mutual_info_classif(x, y, discrete_features=True)[0]
        else:
            mi_score = mutual_info_regression(x, y)[0]
        feature_importances.append((col, mi_score))
    
    feature_importances.sort(key=lambda x: x[1], reverse=True)
    feature_importances_df = pd.DataFrame(
        feature_importances, 
        columns=['Feature', 'Importance']
    )
    
    return mi_df, feature_importances_df

# Example usage:
if __name__ == "__main__":
    df = create_sample_classification_data()
    X = df.drop(columns=['loan_approved'])
    y = df.loan_approved.to_numpy()
    
    mi_df, feature_importances_df = calculate_mutual_info(X, y)
    print("\nMutual Information Matrix:")
    display(mi_df.round(3))
    print("\nFeature Importances:")
    display(feature_importances_df)
