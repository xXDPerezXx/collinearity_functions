import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.preprocessing import KBinsDiscretizer

def calculate_mutual_info(X, y, categorical_features, numeric_features):
    """
    Calculate mutual information between features and with target variable.
    
    Parameters:
    X : pandas DataFrame
        The feature matrix
    y : array-like
        The target variable
    categorical_features : list
        Indices of categorical features
    numeric_features : list
        Indices of numeric features
    """
    # Initialize an empty matrix to store the mutual information scores
    n_features = X.shape[1]
    mi_matrix = np.zeros((n_features, n_features))
    feature_names = list(X.columns)
    
    # Convert DataFrame to numpy array for consistent indexing
    X_array = X.values
    
    # Calculate the mutual information scores between each pair of features
    for i in range(n_features):
        for j in range(n_features):
            if i == j:
                mi_matrix[i, j] = 1.0
            elif i in categorical_features and j in categorical_features:
                mi_matrix[i, j] = mutual_info_classif(
                    X_array[:, i].reshape(-1, 1), 
                    X_array[:, j]
                )[0]
            elif i in numeric_features and j in numeric_features:
                mi_matrix[i, j] = mutual_info_regression(
                    X_array[:, i].reshape(-1, 1), 
                    X_array[:, j]
                )[0]
            elif i in categorical_features and j in numeric_features:
                est = KBinsDiscretizer(n_bins=5, encode='ordinal')
                X_discretized = est.fit_transform(X_array[:, j].reshape(-1, 1))
                mi_matrix[i, j] = mutual_info_classif(
                    X_array[:, i].reshape(-1, 1), 
                    X_discretized.ravel()
                )[0]
            elif i in numeric_features and j in categorical_features:
                est = KBinsDiscretizer(n_bins=5, encode='ordinal')
                X_discretized = est.fit_transform(X_array[:, i].reshape(-1, 1))
                mi_matrix[i, j] = mutual_info_classif(
                    X_discretized, 
                    X_array[:, j]
                )[0]
    
    # Create a DataFrame with the mutual information scores
    mi_df = pd.DataFrame(mi_matrix, index=feature_names, columns=feature_names)
    
    # Create a heatmap with the mutual information scores
    plt.figure(figsize=(10, 8))
    sns.heatmap(mi_df, annot=True, cmap='coolwarm', square=True)
    plt.title('Mutual Information Matrix')
    plt.show()
    
    # Calculate the ranking of all the features based on importance
    feature_importances = []
    for i in range(n_features):
        if i in categorical_features:
            mi_score = mutual_info_classif(
                X_array[:, i].reshape(-1, 1), 
                y
            )[0]
        else:
            mi_score = mutual_info_regression(
                X_array[:, i].reshape(-1, 1), 
                y
            )[0]
        feature_importances.append((feature_names[i], mi_score))
    
    # Sort the feature importances in descending order
    feature_importances.sort(key=lambda x: x[1], reverse=True)
    
    # Create a DataFrame with the feature importances
    feature_importances_df = pd.DataFrame(
        feature_importances, 
        columns=['Feature', 'Importance']
    )
    
    return mi_df, feature_importances_df

# Example usage:
if __name__ == "__main__":
    from sklearn.datasets import load_iris
    iris = load_iris()
    X = pd.DataFrame(iris.data, columns=iris.feature_names)
    y = iris.target
    categorical_features = []
    numeric_features = list(range(X.shape[1]))
    mi_df, feature_importances_df = calculate_mutual_info(
        X, y, categorical_features, numeric_features
    )
    print("\nMutual Information Matrix:")
    print(mi_df)
    print("\nFeature Importances:")
    print(feature_importances_df)
